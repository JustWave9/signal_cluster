import os
import numpy as np
import scipy.io
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from extract_feature import extract_feature  # å‡è®¾ä½ çš„18ç»´ç‰¹å¾å‡½æ•°ä¿å­˜ä¸ºextract_feature.py

def get_true_label(filename):
    return int(filename.split('_')[1])

# ä¸»ç¨‹åºå‚æ•°
folder = r'D:\matrixlab\match_tar\test6s'  # ä¿®æ”¹ä¸ºä½ çš„ä¿¡å·è·¯å¾„
Fs = 1e7  # é‡‡æ ·ç‡

features_list = []
file_list = []

# æå–ç‰¹å¾
for file in tqdm(os.listdir(folder)):
    if file.endswith('.mat'):
        path = os.path.join(folder, file)
        data = scipy.io.loadmat(path)
        if 'baseband_signal' in data:
            signal = data['baseband_signal']
            if signal.ndim > 1:
                signal = signal.flatten()
            feature_vector = extract_feature(signal[np.newaxis, :], Fs)[0]  # å•æ¡ä¿¡å·ï¼Œæå–ä¸€è¡Œ
            features_list.append(feature_vector)
            file_list.append(file)

# ç‰¹å¾æ ‡å‡†åŒ–
X = np.array(features_list)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# KMeansèšç±»
kmeans = KMeans(n_clusters=2, random_state=0)
labels = kmeans.fit_predict(X_scaled)

# è¾“å‡ºèšç±»ç»“æœ
print("æ–‡ä»¶å â†’ èšç±»ç±»åˆ«")
for fname, label in zip(file_list, labels):
    print(f"{fname} â†’ ç±»åˆ« {label}")

# æ„é€ çœŸå®æ ‡ç­¾
y_true = [get_true_label(fname)-1 for fname in file_list]
acc1 = accuracy_score(y_true, labels)
acc2 = accuracy_score(y_true, 1 - labels)
if acc2 > acc1:
    labels = 1 - labels  # æ ‡ç­¾å¯¹é½

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, labels)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Pred 0", "Pred 1"], yticklabels=["True 0", "True 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("KMeans Confusion Matrix")
plt.tight_layout()
plt.show()

# åˆ†ç±»æŠ¥å‘Š
print(classification_report(y_true, labels, target_names=["Class 0", "Class 1"]))



# import os
# import numpy as np
# import scipy.io
# from scipy.fft import fft
# from scipy.signal import correlate, find_peaks
# import scipy.stats as stats
# import pywt
# from sklearn.cluster import KMeans
# from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score
# import matplotlib.pyplot as plt
# import seaborn as sns
#
# def extract_features(signal, Fs):
#     signal = np.asarray(signal).flatten()
#     abs_signal = np.abs(signal)  # å¤ä¿¡å·è½¬ä¸ºæ¨¡å€¼
#     L = len(signal)
#     T = 1 / Fs
#     features = {}
#
#     # ä¸€ã€ç»Ÿè®¡ç‰¹å¾ï¼ˆåŸºäºæ¨¡å€¼ï¼‰
#     features['max'] = np.max(abs_signal)
#     features['min'] = np.min(abs_signal)
#     features['mean'] = np.mean(abs_signal)
#     features['median'] = np.median(abs_signal)
#     features['skewness'] = stats.skew(abs_signal, bias=False)
#     features['kurtosis'] = stats.kurtosis(abs_signal, fisher=False)
#     features['iqr'] = np.percentile(abs_signal, 75) - np.percentile(abs_signal, 25)
#     features['mad_mean'] = np.mean(np.abs(abs_signal - np.mean(abs_signal)))
#     features['mad_median'] = np.median(np.abs(abs_signal - np.median(abs_signal)))
#     features['rms'] = np.sqrt(np.mean(abs_signal**2))
#     features['std'] = np.std(abs_signal)
#     features['var'] = np.var(abs_signal)
#     features['percentile_50'] = np.sum(abs_signal <= np.percentile(abs_signal, 50)) / len(abs_signal)
#
#     # äºŒã€é¢‘è°±ç‰¹å¾ï¼ˆæ¨¡å€¼ï¼‰
#     fft_val = fft(signal)
#     P2 = np.abs(fft_val / L)
#     P1 = P2[:L // 2 + 1]
#     P1[1:-1] *= 2
#     features['fft_mean'] = np.mean(P1)
#     features['fft_max'] = np.max(P1)
#     features['fft_median'] = np.median(P1)
#     peaks, _ = find_peaks(P1)
#     features['fft_base'] = peaks[0] * Fs / L if len(peaks) > 0 else 0
#
#     # ä¸‰ã€å°æ³¢ç‰¹å¾ï¼ˆæ¨¡å€¼ï¼‰
#     coeffs = pywt.wavedec(abs_signal, 'db1', level=5)
#     features['wavelet_abs_mean'] = np.mean([np.mean(np.abs(c)) for c in coeffs])
#     features['wavelet_std'] = np.mean([np.std(c) for c in coeffs])
#     # features['wavelet_var'] = np.mean([np.var(c) for c in coeffs])
#
#     # å››ã€å·®åˆ†ç‰¹å¾ï¼ˆæ¨¡å€¼ï¼‰
#     diff = np.diff(abs_signal)
#     features['diff_mean'] = np.mean(diff)
#     # features['diff_abs_mean'] = np.mean(np.abs(diff))
#     features['diff_median'] = np.median(diff)
#     features['diff_abs_median'] = np.median(np.abs(diff))
#     # features['diff_sum'] = np.sum(np.abs(diff))
#
#     # äº”ã€ç†µï¼ˆæ¨¡å€¼ï¼‰
#     hist, _ = np.histogram(abs_signal, bins=256, density=True)
#     hist = hist[hist > 0]
#     features['entropy'] = -np.sum(hist * np.log2(hist))
#
#     # å…­ã€å‡ ä½•ç‰¹å¾ï¼ˆæ¨¡å€¼ï¼‰
#     features['x_dist_peak_valley'] = np.abs(np.argmax(abs_signal) - np.argmin(abs_signal)) * T
#     features['area'] = np.sum(abs_signal)
#     # features['num_max_peaks'] = len(find_peaks(abs_signal)[0])
#     # features['num_min_peaks'] = len(find_peaks(-abs_signal)[0])
#     features['zero_cross_rate'] = np.sum((abs_signal[:-1] * abs_signal[1:] < 0) | (abs_signal[:-1] == 0))
#
#     return features
#
# # ä»æ–‡ä»¶åä¸­æå–çœŸå®æ ‡ç­¾ï¼ˆç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿åçš„æ•°å­—ï¼‰
# def get_true_label(filename):
#     return int(filename.split('_')[1])
#
#
# # === ä¸»ç¨‹åº ===
# folder = (r'D:\matrixlab\match_tar\test4m'
#           r'')  # ä¿®æ”¹ä¸ºä½ çš„ä¿¡å·è·¯å¾„
# Fs = 1e7  # é‡‡æ ·ç‡
#
# features_list = []
# file_list = []
#
# for file in os.listdir(folder):
#     if file.endswith('.mat'):
#         path = os.path.join(folder, file)
#         data = scipy.io.loadmat(path)
#         if 'baseband_signal' in data:
#             signal = data['baseband_signal'].flatten()
#             features = extract_features(signal, Fs)
#             features_list.append(list(features.values()))
#             file_list.append(file)
#             # âœ… æ‰“å°æ¯ä¸ªç‰¹å¾
#             print(f"\nğŸ“‚ æ–‡ä»¶: {file}")
#             for k, v in features.items():
#                 print(f"  {k:<25}: {v:.4e}")
#
# # è½¬æˆæ•°ç»„å¹¶æ ‡å‡†åŒ–
# X = np.array(features_list)
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)
#
# # KMeansèšç±»
# kmeans = KMeans(n_clusters=2, random_state=0)
# labels = kmeans.fit_predict(X_scaled)
#
# # è¾“å‡ºèšç±»ç»“æœ
# print("æ–‡ä»¶å â†’ èšç±»ç±»åˆ«")
# for fname, label in zip(file_list, labels):
#     print(f"{fname} â†’ ç±»åˆ« {label}")
#
# # æ„é€ çœŸå®æ ‡ç­¾åˆ—è¡¨
# y_true = [get_true_label(fname)-1 for fname in file_list]
# # å¯¹é½KMeansè¾“å‡ºæ ‡ç­¾ï¼ˆé˜²æ­¢0/1é¡ºåºé¢ å€’ï¼‰
# acc1 = accuracy_score(y_true, labels)
# acc2 = accuracy_score(y_true, 1 - labels)
# if acc2 > acc1:
#     labels = 1 - labels  # æ ‡ç­¾åè½¬å¯¹é½
#
# # æ··æ·†çŸ©é˜µ
# cm = confusion_matrix(y_true, labels)
#
# # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
# plt.figure(figsize=(6, 5))
# sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
#             xticklabels=["Pred 0", "Pred 1"],
#             yticklabels=["True 0", "True 1"])
# plt.xlabel("Predicted Label")
# plt.ylabel("True Label")
# plt.title("KMeans Confusion Matrix")
# plt.tight_layout()
# plt.show()
#
# # åˆ†ç±»æŠ¥å‘Š
# print(classification_report(y_true, labels, target_names=["Class 0", "Class 1"]))
